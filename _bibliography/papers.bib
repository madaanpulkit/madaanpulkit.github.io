---
---

@book{einstein1920relativity,
  title={Relativity: the Special and General Theory},
  author={Einstein, Albert},
  year={1920},
  publisher={Methuen & Co Ltd},
  html={relativity.html}
}

@book{einstein1956investigations,
  bibtex_show={true},
  title={Investigations on the Theory of the Brownian Movement},
  author={Einstein, Albert},
  year={1956},
  publisher={Courier Corporation},
  preview={brownian-motion.gif}
}

@article{einstein1950meaning,
  abbr={AJP},
  bibtex_show={true},
  title={The meaning of relativity},
  author={Einstein, Albert and Taub, AH},
  journal={American Journal of Physics},
  volume={18},
  number={6},
  pages={403--404},
  year={1950},
  publisher={American Association of Physics Teachers}
}

@article{PhysRev.47.777,
  abbr={PhysRev},
  title={Can Quantum-Mechanical Description of Physical Reality Be Considered Complete?},
  author={Einstein, A. and Podolsky, B. and Rosen, N.},
  abstract={In a complete theory there is an element corresponding to each element of reality. A sufficient condition for the reality of a physical quantity is the possibility of predicting it with certainty, without disturbing the system. In quantum mechanics in the case of two physical quantities described by non-commuting operators, the knowledge of one precludes the knowledge of the other. Then either (1) the description of reality given by the wave function in quantum mechanics is not complete or (2) these two quantities cannot have simultaneous reality. Consideration of the problem of making predictions concerning a system on the basis of measurements made on another system that had previously interacted with it leads to the result that if (1) is false then (2) is also false. One is thus led to conclude that the description of reality as given by a wave function is not complete.},
  journal={Phys. Rev.},
  volume={47},
  issue={10},
  pages={777--780},
  numpages={0},
  year={1935},
  month={May},
  publisher=aps,
  doi={10.1103/PhysRev.47.777},
  url={http://link.aps.org/doi/10.1103/PhysRev.47.777},
  html={https://journals.aps.org/pr/abstract/10.1103/PhysRev.47.777},
  pdf={example_pdf.pdf},
  altmetric={248277},
  dimensions={true},
  google_scholar_id={qyhmnyLat1gC},
  selected={true}
}

@article{einstein1905molekularkinetischen,
  title={{\"U}ber die von der molekularkinetischen Theorie der W{\"a}rme geforderte Bewegung von in ruhenden Fl{\"u}ssigkeiten suspendierten Teilchen},
  author={Einstein, A.},
  journal={Annalen der physik},
  volume={322},
  number={8},
  pages={549--560},
  year={1905},
  publisher={Wiley Online Library}
}

@article{einstein1905movement,
  abbr={Ann. Phys.},
  title={Un the movement of small particles suspended in statiunary liquids required by the molecular-kinetic theory 0f heat},
  author={Einstein, A.},
  journal={Ann. Phys.},
  volume={17},
  pages={549--560},
  year={1905}
}

@article{einstein1905electrodynamics,
  title={On the electrodynamics of moving bodies},
  author={Einstein, A.},
  year={1905}
}

@book{przibram1967letters,
  bibtex_show={true},
  title={Letters on wave mechanics},
  author={Einstein, Albert and Schr√∂dinger, Erwin and Planck, Max and Lorentz, Hendrik Antoon and Przibram, Karl},
  year={1967},
  publisher={Vision},
  preview={wave-mechanics.gif}
}

@inproceedings{madaan-sadat-2020-multilingual,
  bibtex_show={true},
  abbr={LREC},
  title= {Multilingual Neural Machine Translation involving {I}ndian Languages},
  author= {Madaan, Pulkit and Sadat, Fatiha},
  booktitle= {Proceedings of the WILDRE5{--} 5th Workshop on Indian Language Data: Resources and Evaluation},
  month= {may},
  year= {2020},
  address= {Marseille, France},
  publisher= {European Language Resources Association (ELRA)},
  url= {https://www.aclweb.org/anthology/2020.wildre-1.6},
  pages= {29--32},
  abstract= {Neural Machine Translations (NMT) models are capable of translating a single bilingual pair and require a new model for each new language pair. Multilingual Neural Machine Translation models are capable of translating multiple language pairs, even pairs which it hasn{'}t seen before in training. Availability of parallel sentences is a known problem in machine translation. Multilingual NMT model leverages information from all the languages to improve itself and performs better. We propose a data augmentation technique that further improves this model profoundly. The technique helps achieve a jump of more than 15 points in BLEU score from the multilingual NMT model. A BLEU score of 36.2 was achieved for Sindhi{--}English translation, which is higher than any score on the leaderboard of the LoResMT SharedTask at MT Summit 2019, which provided the data for the experiments.},
  language= {English},
  ISBN= {979-10-95546-67-2},
  selected= {true},
  pdf= {nmt_paper.pdf}}

@article{madaan2019deep,
  bibtex_show={true},
  abbr = {Thesis},
  title= {Deep mean shift clustering},
  author= {Madaan, Pulkit and Maiti, Abhishek and Anand, Saket and Mittal, Sushil},
  year= {2019},
  publisher= {IIIT-Delhi},
  url= {https://repository.iiitd.edu.in/jspui/handle/123456789/915},
  language= {English},
  abstract= {We use Mean Shift clustering in the latent space of an auto-encoder to have a better representation of the data and a more structured latent space. Instead of just using the mode of the distribution calculated using kernel density estimates, we use trajectories of data points leading to the modes to better model the basin of attraction of each mode. This helps in better structuring of the latent space and results in a more inferential model. Since mean-shift can be modelled as an RNN-block [16] our method is end-to-end trainable. Tuning the bandwidth of mean-shift gives us the flexibility of clustering the latent space on different hierarchical levels. We modify the original trajectory based LSTM model by incorporating a discounting mechanism. We modified the mean shift implementation by using a fixed kernel for the mean shift iteratiosn. We also apply a new loss (Support Set Loss) to penalize the clusters made on the latent space. This uses the trajectories of the points segregated into groups which ended up in the same mode and those which didn't. We have used this loss function in both semi-supervised and unsupervised fashion. In the end, we also propose a model which uses Contrastive Predictive Coding loss, given in [23] in the latent space as well as a regularizer for the encoding network model.},
  selected= {true},
  pdf= {thesis_paper.pdf}}

