---
---

@inproceedings{ramesh-etal-2025-synthtexteval,
  bibtex_show={true},
  abbr={EMNLP},
  selected = {true},
  title={{S}ynth{T}ext{E}val: Synthetic Text Data Generation and Evaluation for High-Stakes Domains},
  author={Ramesh, Krithika and Smolyak, Daniel and Zhao, Zihao and Gandhi, Nupoor and Agarwal, Ritu and Bjarnad{\'o}ttir, Margr{\'e}t V. and Field, Anjalie},
  editor={Habernal, Ivan and Schulam, Peter and Tiedemann, J{\"o}rg},
  booktitle={Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
  month=nov,
  year={2025},
  address={Suzhou, China},
  publisher={Association for Computational Linguistics},
  url={https://aclanthology.org/2025.emnlp-demos.35/},
  doi={10.18653/v1/2025.emnlp-demos.35},
  html={https://aclanthology.org/2025.emnlp-demos.35/},
  pages={487--499},
  ISBN={979-8-89176-334-0},
  abstract={We present SynthTextEval, a toolkit for conducting comprehensive evaluations of synthetic text. The fluency of large language model (LLM) outputs has made synthetic text potentially viable for numerous applications, such as reducing the risks of privacy violations in the development and deployment of AI systems in high-stakes domains. Realizing this potential, however, requires principled consistent evaluations of synthetic data across multiple dimensions: its utility in downstream systems, the fairness of these systems, the risk of privacy leakage, general distributional differences from the source text, and qualitative feedback from domain experts. SynthTextEval allows users to conduct evaluations along all of these dimensions over synthetic data that they upload or generate using the toolkit{'}s generation module. While our toolkit can be run over any data, we highlight its functionality and effectiveness over datasets from two high-stakes domains: healthcare and law. By consolidating and standardizing evaluation metrics, we aim to improve the viability of synthetic text, and in-turn, privacy-preservation in AI development.},
}

@inproceedings{https://doi.org/10.48550/arxiv.2208.06359,
  bibtex_show={true},
  abbr={NeurIPS},
  title={A Case for Rejection in Low Resource ML Deployment},
  author = {White, Jerome and Madaan, Pulkit and Shenoy, Nikhil and Agnihotri, Apoorv and Sharma, Makkunda and Doshi, Jigar},
  booktitle={NeurIPS 2025 Workshop for Challenges in Deploying and Monitoring Machine Learning Systems},
  year={2022},
  selected={true},
  html = {https://neurips.cc/virtual/2022/62142},
  abstract = {Building reliable AI decision support systems requires a robust set of data on which to train models; both with respect to quantity and diversity. Obtaining such datasets can be difficult in resource limited settings, or for applications in early stages of deployment. Sample rejection is one way to work around this challenge, however much of the existing work in this area is ill-suited for such scenarios. This paper substantiates that position and proposes a simple solution as a proof of concept baseline.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  pdf = {https://arxiv.org/ftp/arxiv/papers/2208/2208.06359.pdf}
}

@inproceedings{madaan-sadat-2020-multilingual,
  bibtex_show={true},
  abbr= {LREC},
  title= {Multilingual Neural Machine Translation involving Indian Languages},
  author= {Madaan, Pulkit and Sadat, Fatiha},
  booktitle= {Proceedings of the WILDRE5{--} 5th Workshop on Indian Language Data: Resources and Evaluation},
  month= {may},
  year= {2020},
  address= {Marseille, France},
  publisher= {European Language Resources Association (ELRA)},
  url= {https://www.aclweb.org/anthology/2020.wildre-1.6},
  pages= {29--32},
  abstract= {Neural Machine Translations (NMT) models are capable of translating a single bilingual pair and require a new model for each new language pair. Multilingual Neural Machine Translation models are capable of translating multiple language pairs, even pairs which it hasn't seen before in training. Availability of parallel sentences is a known problem in machine translation. Multilingual NMT model leverages information from all the languages to improve itself and performs better. We propose a data augmentation technique that further improves this model profoundly. The technique helps achieve a jump of more than 15 points in BLEU score from the multilingual NMT model. A BLEU score of 36.2 was achieved for Sindhi{--}English translation, which is higher than any score on the leaderboard of the LoResMT SharedTask at MT Summit 2019, which provided the data for the experiments.},
  language= {English},
  ISBN= {979-10-95546-67-2},
  selected= {true},
  pdf= {https://aclanthology.org/2020.wildre-1.6.pdf}
 }

@article{madaan2019deep,
  bibtex_show={true},
  abbr= {Thesis},
  title= {Deep mean shift clustering},
  author= {Madaan, Pulkit and Maiti, Abhishek and Anand, Saket and Mittal, Sushil},
  year= {2019},
  publisher= {IIIT-Delhi},
  url= {https://repository.iiitd.edu.in/jspui/handle/123456789/915},
  language= {English},
  abstract= {We use Mean Shift clustering in the latent space of an auto-encoder to have a better representation of the data and a more structured latent space. Instead of just using the mode of the distribution calculated using kernel density estimates, we use trajectories of data points leading to the modes to better model the basin of attraction of each mode. This helps in better structuring of the latent space and results in a more inferential model. Since mean-shift can be modelled as an RNN-block our method is end-to-end trainable. Tuning the bandwidth of mean-shift gives us the flexibility of clustering the latent space on different hierarchical levels. We modify the original trajectory based LSTM model by incorporating a discounting mechanism. We modified the mean shift implementation by using a fixed kernel for the mean shift iteratiosn. We also apply a new loss (Support Set Loss) to penalize the clusters made on the latent space. This uses the trajectories of the points segregated into groups which ended up in the same mode and those which didn't. We have used this loss function in both semi-supervised and unsupervised fashion. In the end, we also propose a model which uses Contrastive Predictive Coding loss, in the latent space as well as a regularizer for the encoding network model.},
  selected= {true},
  pdf= {https://repository.iiitd.edu.in/jspui/bitstream/handle/123456789/915/Pulkit%20Madaan-2016257%2c%20Abhishek%20Maiti-2016005.pdf}
 }
